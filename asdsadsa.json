{"topics":[["given string","blow given","gethostnameofip blow","netutils gethostnameofip","because regex","blow","check given","doesn't check","gethostnameofip","given","match optionally","netutils","optionally requires","regex match","requires semicolon","semicolon doesn't","string because"],["fails","testzkfailovercontroller fails","testzkfailovercontroller"],["string","asserting wrong","testgetblocks asserting","wrong string","asserting","getblockswithexception resulting","resulting failure","string getblockswithexception","testgetblocks","wrong"],["common version","common","hadoop requires","latest stable","let's latest","other ftpfs","requires common","stable version","update common","update","upgraded let's","version other","version upgraded","version"],["configure script","build","during build","script"]],"docs":[["Improper log mesage when blockreport interval comapred with initial delay",{"content":"Log message is telling if initialDelay is more than blockReportInterval setting to 0.But actuall check is greaterthan and equal..like following..It is misleading initail I thought if itis equal then initailBr wn't set to zero.{code}if (initBRDelay >= blockReportInterval) {      initBRDelay = 0;      DataNode.LOG.info(\"dfs.blockreport.initialDelay is greater than \" +          \"dfs.blockreport.intervalMsec.\" + \" Setting initial delay to 0 msec:\");    }{code}","wordsAmount":39,"comments":[{"name":"Patch addressing the improper log message.","sentiment":0},{"name":"No testcase added as change is done only in log message. ","sentiment":-0.296}],"sentiment":-0.3612,"terms":[]}],["Investigate uses of FileUtil and functional correctness based on current use cases",{"content":"The current Windows patch replaces symlink with copy. This jira tracks understanding the implications of this change and others like it on expected functionality.","wordsAmount":24,"comments":[{"name":"Multiple other jiras have superceded this.","sentiment":0}],"sentiment":0.3612,"terms":[]}],["invalid hadoop-auth cookies should trigger authentication if info is avail before returning HTTP 401",{"content":"WebHdfs gives out cookies. But when the client passes them back, it'd sometimes reject them and return a HTTP 401 instead. (\"Sometimes\" as in after a restart.) The interesting thing is that if the client doesn't pass the cookie back, WebHdfs will be totally happy.The correct behaviour should be to ignore the cookie if it looks invalid, and attempt to proceed with the request handling.I haven't tried HttpFs to see whether it handles restart better.Reproducing it with curl:{noformat}###################################################### Initial curl. Storing cookie to file.####################################################[root@vbox2 ~]# curl -c /tmp/webhdfs.cookie -i 'http://localhost:50070/webhdfs/v1/?op=LISTSTATUS&user.name=bcwalrus'HTTP/1.1 200 OKContent-Type: application/jsonExpires: Thu, 01-Jan-1970 00:00:00 GMTSet-Cookie: hadoop.auth=\"u=bcwalrus&p=bcwalrus&t=simple&e=1333614686366&s=z2w5xpFlufnnEoOHxVRiXqxwtqM=\";Path=/Content-Length: 597Server: Jetty(6.1.26){\"FileStatuses\":{\"FileStatus\":[{\"accessTime\":0,\"blockSize\":0,\"group\":\"supergroup\",\"length\":0,\"modificationTime\":1333577906198,\"owner\":\"mapred\",\"pathSuffix\":\"tmp\",\"permission\":\"1777\",\"replication\":0,\"type\":\"DIRECTORY\"},{\"accessTime\":0,\"blockSize\":0,\"group\":\"supergroup\",\"length\":0,\"modificationTime\":1333577511848,\"owner\":\"hdfs\",\"pathSuffix\":\"user\",\"permission\":\"1777\",\"replication\":0,\"type\":\"DIRECTORY\"},{\"accessTime\":0,\"blockSize\":0,\"group\":\"supergroup\",\"length\":0,\"modificationTime\":1333428745116,\"owner\":\"mapred\",\"pathSuffix\":\"var\",\"permission\":\"755\",\"replication\":0,\"type\":\"DIRECTORY\"}]}}###################################################### Another curl. Using the cookie jar.####################################################[root@vbox2 ~]# curl -b /tmp/webhdfs.cookie -i 'http://localhost:50070/webhdfs/v1/?op=LISTSTATUS&user.name=bcwalrus'HTTP/1.1 200 OKContent-Type: application/jsonContent-Length: 597Server: Jetty(6.1.26){\"FileStatuses\":{\"FileStatus\":[{\"accessTime\":0,\"blockSize\":0,\"group\":\"supergroup\",\"length\":0,\"modificationTime\":1333577906198,\"owner\":\"mapred\",\"pathSuffix\":\"tmp\",\"permission\":\"1777\",\"replication\":0,\"type\":\"DIRECTORY\"},{\"accessTime\":0,\"blockSize\":0,\"group\":\"supergroup\",\"length\":0,\"modificationTime\":1333577511848,\"owner\":\"hdfs\",\"pathSuffix\":\"user\",\"permission\":\"1777\",\"replication\":0,\"type\":\"DIRECTORY\"},{\"accessTime\":0,\"blockSize\":0,\"group\":\"supergroup\",\"length\":0,\"modificationTime\":1333428745116,\"owner\":\"mapred\",\"pathSuffix\":\"var\",\"permission\":\"755\",\"replication\":0,\"type\":\"DIRECTORY\"}]}}###################################################### Restart NN.####################################################[root@vbox2 ~]# /etc/init.d/hadoop-hdfs-namenode restartStopping Hadoop namenode:                                  [  OK  ]stopping namenodeStarting Hadoop namenode:                                  [  OK  ]starting namenode, logging to /var/log/hadoop-hdfs/hadoop-hdfs-namenode-vbox2.out###################################################### Curl using cookie jar gives error.####################################################[root@vbox2 ~]# curl -b /tmp/webhdfs.cookie -i 'http://localhost:50070/webhdfs/v1/?op=LISTSTATUS&user.name=bcwalrus'HTTP/1.1 401 org.apache.hadoop.security.authentication.util.SignerException: Invalid signatureContent-Type: text/html; charset=iso-8859-1Set-Cookie: hadoop.auth=;Path=/;Expires=Thu, 01-Jan-1970 00:00:00 GMTCache-Control: must-revalidate,no-cache,no-storeContent-Length: 1520Server: Jetty(6.1.26)<html><head><meta http-equiv=\"Content-Type\" content=\"text/html; charset=ISO-8859-1\"/><title>Error 401 org.apache.hadoop.security.authentication.util.SignerException: Invalid signature</title></head><body><h2>HTTP ERROR 401</h2><p>Problem accessing /webhdfs/v1/. Reason:<pre>    org.apache.hadoop.security.authentication.util.SignerException: Invalid signature</pre></p><hr /><i><small>Powered by Jetty://</small></i><br/>                                                ...###################################################### Curl without cookie jar is ok.####################################################[root@vbox2 ~]# curl -i 'http://localhost:50070/webhdfs/v1/?op=LISTSTATUS&user.name=bcwalrus'HTTP/1.1 200 OKContent-Type: application/jsonExpires: Thu, 01-Jan-1970 00:00:00 GMTSet-Cookie: hadoop.auth=\"u=bcwalrus&p=bcwalrus&t=simple&e=1333614995947&s=IXSvPIDbNrqmZryivGeoey6Kjwo=\";Path=/Content-Length: 597Server: Jetty(6.1.26){\"FileStatuses\":{\"FileStatus\":[{\"accessTime\":0,\"blockSize\":0,\"group\":\"supergroup\",\"length\":0,\"modificationTime\":1333577906198,\"owner\":\"mapred\",\"pathSuffix\":\"tmp\",\"permission\":\"1777\",\"replication\":0,\"type\":\"DIRECTORY\"},{\"accessTime\":0,\"blockSize\":0,\"group\":\"supergroup\",\"length\":0,\"modificationTime\":1333577511848,\"owner\":\"hdfs\",\"pathSuffix\":\"user\",\"permission\":\"1777\",\"replication\":0,\"type\":\"DIRECTORY\"},{\"accessTime\":0,\"blockSize\":0,\"group\":\"supergroup\",\"length\":0,\"modificationTime\":1333428745116,\"owner\":\"mapred\",\"pathSuffix\":\"var\",\"permission\":\"755\",\"replication\":0,\"type\":\"DIRECTORY\"}]}}{noformat}","wordsAmount":305,"comments":[{"name":"the AuthenticationFilter logic now attempts to authenticate the request without doing a HTTP 401 if there is info avail in the request to perform the authentication.","sentiment":0},{"name":"test failure is unrelated","sentiment":-0.5106},{"name":"+1  looks good\r\n\r\nStyle nit: the catch and else clause go on the same line as the bracket","sentiment":0.4404},{"name":"committed to trunk and branch-2","sentiment":0.2732},{"name":"we need backport for hadoop 1","sentiment":0},{"name":"patch for branch-1","sentiment":0},{"name":"committed to branch-1","sentiment":0.2732},{"name":"Just a question, did you test if MR jobs gracefully handle the re-auth response?  I'm presuming the job won't have the credentials for a re-auth, so hopefully it makes the job \"gracefully\" fail? ","sentiment":0.5791}],"sentiment":-0.2641,"terms":[]}],["Auto-HA: automatically scope znode by nameservice ID",{"content":"Talking to some folks who work on operations/deployment, they pointed out that it would make sense to automatically include the nameservice ID in the base znode used for automatic failover. For example, even though the \"root znode\" is \"/hadoop-ha\", we should put the znodes for a nameservice \"my-ha-cluster\" within \"/hadoop-ha/my-ha-cluster\". This allows federated setups to work with no additional configuration.","wordsAmount":34,"comments":[{"name":"Attached patch scopes the znodes by the configured nameservice ID. So, if the configured parent znode is /hadoop-ha, then it will work inside /hadoop-ha/<nameservice>/ without having to manually configure the parent znode per-quorum.","sentiment":0},{"name":"+1, the patch looks good to me. Good on you, Todd.","sentiment":0.7003},{"name":"Thanks for the quick review. I ran all the ZKFC tests and committed this to the branch.","sentiment":0.6124}],"sentiment":-0.296,"terms":[]}],["Improve comments on ByteBufferReadable.read",{"content":"There are a couple of ways in which the comment on ByteBufferReadable.read can be improved. Since this is a public-facing API with potentially many implementations, it's worth taking the time to get it right.* We should describe what can become of the byte buffer state in the case of an exception (is the limit changed? where's the position?). For the DFSInputStream implementation, position and limit are unchanged if there is an error, but I don't think that's the right think to mandate for all implementations. * We should mention explicitly that 0-byte reads are legitimate - this is particularly important in light of HDFS-3110 which detects support for direct reads by issuing a 0-byte read from libhdfs. ","wordsAmount":57,"comments":[{"name":"Patch against trunk. No tests included, because it's a doc update.","sentiment":-0.296},{"name":"I'm going to go out on a limb and say the test failure is not as a result of this patch :)","sentiment":-0.0772},{"name":"Patch looks pretty good to me, Henry. One little comment:\r\n\r\n# \"After the call, buf.position() and buf.limit() should be unchanged...\" - perhaps change this to \"after a successful call\" ?","sentiment":0.872},{"name":"Seems reasonable - patch includes Aaron's suggested rewording. ","sentiment":0},{"name":"+1, the patch looks good to me. I'm going to commit this momentarily.","sentiment":0.6249},{"name":"I've just committed this to trunk. Thanks a lot for the contribution, Hank.","sentiment":0.6124},{"name":"Committed to branch-2 as well.","sentiment":0.4939}],"sentiment":0.802,"terms":[]}],["NetUtils#getHostNameOfIP blows up if given ip:port string w/o port",{"content":"NetUtils#getHostNameOfIP blows up if not given a string of form ip:port, because the regex matches the port optionally but the code requires a semicolon. It also doesn't check the given string for null.","wordsAmount":23,"comments":[{"name":"Patch attached. Now accepts ip or ip:port, does not accept null, ip:, etc. Added a test for this method.","sentiment":0.0299},{"name":"+1, the patch looks good to me.","sentiment":0.4404},{"name":"Thanks ATM, I've committed this and merged to branch-2.","sentiment":0.6124}],"sentiment":0,"terms":["netutils","gethostnameofip","blow","given","string","netutils gethostnameofip","gethostnameofip blow","blow given","given string","string because","because regex","regex match","match optionally","optionally requires","requires semicolon","semicolon doesn't","doesn't check","check given"]}],["bin/hadoop should allow callers to set jsvc pidfile even when not-detached",{"content":"it would be nice if the jsvc pid file were properly namespaced in /var/run to avoid collisions with other jsvc instancesjsvc uses /var/run/jsvc.pid for the datanode pid file. If that's configurable, it should be configured: /var/run/jsvc.pid is sorely not namespaced.if [ \"$_HADOOP_DAEMON_DETACHED\" = \"true\" ]; then      _JSVC_FLAGS=\"-pidfile $_HADOOP_DAEMON_PIDFILE                  -errfile &1                  -outfile $_HADOOP_DAEMON_OUT\"    else      # Even though we are trying to run a non-detached datanode,      # jsvc will not write to stdout/stderr, so we have to pipe      # it and tail the logfile.      log_path=/tmp/jsvc_${COMMAND}.$$      _JSVC_FLAGS=\"-nodetach                   -errfile &1                   -outfile $log_path\"      echo Non-detached jsvc output piping to: $log_path      touch $log_path      tail -f $log_path &    fiAnd the relevant argument is '-pidfile' (http://linux.die.net/man/1/jsvc).","wordsAmount":58,"comments":[{"name":"This doesn't apply to branch-1 or branch-2.","sentiment":0}],"sentiment":-0.4588,"terms":[]}],["TestZKFailoverController fails",{"content":"","wordsAmount":3,"comments":[{"name":"Duplicate of HDFS-8218.","sentiment":0}],"sentiment":0,"terms":["testzkfailovercontroller","fails","testzkfailovercontroller fails"]}],["address log4j.properties inconsistencies btw main and template dirs",{"content":"DRFAAUDIT is in ./hadoop-common-project/hadoop-common/src/main/packages/templates/conf/log4j.properties but not in ./hadoop-common-project/hadoop-common/src/main/conf/log4j.properties (although it's used in hadoop-env.sh)lesser issues but I noticed:I see MRAUDIT commented out of main, but it's uncommented in template. (the conversion pattern also differs). afaict the other appenders (JSA, MRAUDIT, TLA, etc...) are all still being used and should not be removed.Should we be renaming some of these? JSA is being used by the history server for example.","wordsAmount":47,"comments":[{"name":"I think we should just use the values from templates/conf/log4j.properties in main/conf/log4j.properties, the former look the most up2date, and they're mostly the same (ie just different order in some places, and removing things that are commented out). ","sentiment":0.4019},{"name":"I checked over the template and it seemed a superset of main, so I've gone ahead and done as Eli suggested. I was able to start a cluster w/o issues and run a basic jobs. The generated logs looked fine to me.","sentiment":0.2023},{"name":"+1  looks good","sentiment":0.4404},{"name":"Forgot to mention, the test failure is unrelated. I've committed and merged to branch-23. Thanks Pat!","sentiment":0.2481}],"sentiment":0,"terms":[]}],["make hadoop script recognize a full set of deprecated commands",{"content":"bin/hadoop launcher script does a nice job of recognizing deprecated usage and vectoring users towards the proper command line tools (hdfs, mapred). It would be nice if we can take care of the following deprecated commands that don't get the same special treatment:{noformat}  oiv                  apply the offline fsimage viewer to an fsimage  dfsgroups            get the groups which users belong to on the Name Node  mrgroups             get the groups which users belong to on the Job Tracker  mradmin              run a Map-Reduce admin client  jobtracker           run the MapReduce job Tracker node  tasktracker          run a MapReduce task Tracker node{noformat}Here's what I propos to do with them:  # oiv        -- issue DEPRECATED warning and run hdfs oiv  # dfsgroups  -- issue DEPRECATED warning and run hdfs groups  # mrgroups   -- issue DEPRECATED warning and run mapred groups  # mradmin    -- issue DEPRECATED warning and run yarn rmadmin  # jobtracker -- issue DEPRECATED warning and do nothing  # tasktracker-- issue DEPRECATED warning and do nothingThoughts?","wordsAmount":87,"comments":[{"name":"+1","sentiment":0},{"name":"thanks Roman. committed to trunk and branch-2","sentiment":0.6124}],"sentiment":-0.34,"terms":["script"]}],["TestGetBlocks is asserting on a wrong string",{"content":"TestGetBlocks is asserting on a wrong string in getBlocksWithException() resulting in a test failure.","wordsAmount":13,"comments":[{"name":"Hi Hari, I believe this is a duplicate of HDFS-3143. Do you agree?","sentiment":0.3612},{"name":"Sure looks like a dup.","sentiment":0.5859},{"name":"Resolving as a duplicate of HDFS-3143.","sentiment":0.3818}],"sentiment":-0.7506,"terms":["testgetblocks","asserting","wrong","string","testgetblocks asserting","asserting wrong","wrong string","string getblockswithexception","getblockswithexception resulting","resulting failure"]}],["Update commons-net version to 3.1",{"content":"HADOOP-8210 requires the commons-net version be upgraded. Let's bump it to the latest stable version. The only other user is FtpFs.","wordsAmount":15,"comments":[{"name":"Patch attached.","sentiment":0},{"name":"Arg, I guess changes hadoop-project/pom.xml don't pass test-patch. I'll run tests locally.","sentiment":0},{"name":"+1, assuming you've done a full build locally and run ftpfs-related tests. (are there any such? I can't seem to find any, since HDFS-441 removed it from HDFS but HADOOP-6119 never re-committed it in Common)","sentiment":0},{"name":"I took a stab at restoring HADOOP-6119, I got far enough to load the relevant FTP libraries and start the server but rat holed on setting up user/permissions. Was able to verify that commons-net 3 FtpClient was able to connect to and list the server.","sentiment":-0.2023},{"name":"Thanks Todd, I've committed this.","sentiment":0.6124}],"sentiment":0.296,"terms":["update","common","version","update common","common version","hadoop requires","requires common","version upgraded","upgraded let's","let's latest","latest stable","stable version","version other","other ftpfs"]}],["Disallow self failover",{"content":"It is currently possible for users to make a standby NameNode failover to itself and become active. We shouldn't allow this to happen in case operators mistype and miss the fact that there are now two active NNs.{noformat}bash-4.1$ hdfs haadmin -ns ha-nn-uri -failover nn2 nn2Failover from nn2 to nn2 successful{noformat}After the failover above, nn2 will be active.","wordsAmount":28,"comments":[{"name":"Patch attached.","sentiment":0},{"name":"Updated patch rebased on trunk.","sentiment":0},{"name":"Looks good. I just reverted HADOOP-8193 since it caused some other test failures, but when it is recommitted we can commit this. +1 in advance","sentiment":0.4118},{"name":"Thanks Todd. I've committed this and merged.","sentiment":0.6124}],"sentiment":0.7035,"terms":[]}],["createproxy() in TestHealthMonitor is throwing NPE",{"content":"Looking at the test log output, createproxy in testhealthmonitor is triggering NPE resulting null proxy. This creates other test failures.2012-03-24 22:16:11,591 FATAL ha.HealthMonitor (HealthMonitor.java:uncaughtException(268)) - Health monitor failedjava.lang.NullPointerException        at org.apache.hadoop.ha.TestHealthMonitor$1.createProxy(TestHealthMonitor.java:75)        at org.apache.hadoop.ha.HealthMonitor.tryConnect(HealthMonitor.java:171)        at org.apache.hadoop.ha.HealthMonitor.loopUntilConnected(HealthMonitor.java:158)        at org.apache.hadoop.ha.HealthMonitor.access$500(HealthMonitor.java:52)        at org.apache.hadoop.ha.HealthMonitor$MonitorDaemon.run(HealthMonitor.java:278)","wordsAmount":47,"comments":[{"name":"I think this is dup of HADOOP-8204","sentiment":0}],"sentiment":-0.4824,"terms":[]}],["create the configure script for native compilation as part of the build",{"content":"configure script is checked into svn and its not regenerated during build. Ideally configure scritp should not be checked into svn and instead should be generated during build using autoreconf.","wordsAmount":28,"comments":[{"name":"this patch creates the src/native/configure script and update patches the build.xml to create the configure script inside src/native if its not there.","sentiment":0.4939},{"name":"Giri, isn't this the issue that causes MR to fail in 1.0.2-rc1 if Snappy compression is configured on?  If so, it needs to be marked as a bug, not an improvement.  Thanks.","sentiment":-0.8119},{"name":"+1\r\n\r\nWe should systematically remove all of the autoconf/automake files and regenerate them in the build directory, but this is a step in the right direction.","sentiment":0},{"name":"Committing to branch-1.0.2, branch-1.0, and branch-1.\r\nThanks, Giri!","sentiment":0.5411},{"name":"Confirmed after applying this fix, the only difference in the resulting src/native/configure file was one added block of script related to Snappy.","sentiment":-0.4404}],"sentiment":0.4215,"terms":["script","build","configure script","during build"]}],["Remove HADOOP_[JOBTRACKER|TASKTRACKER]_OPTS ",{"content":"The HADOOP_[JOBTRACKER|TASKTRACKER]_OPTS env variables are no longer in trunk/23 since there's no MR1 implementation and the tests don't use them. This makes the patch for HADOOP-8149 easier.","wordsAmount":21,"comments":[{"name":"Patch attached.","sentiment":0},{"name":"+1","sentiment":0},{"name":"No tests because it just removes an unused env var.\r\nThanks Tom. I've committed this and merged to branch-23.","sentiment":0.4215}],"sentiment":-0.1531,"terms":[]}],["Configuration logs WARNs on every use of a deprecated key",{"content":"The logic to do print a warning only once per deprecated key does not work:{code}2012-03-21 22:32:58,121  WARN Configuration:661 - user.name is deprecated. Instead, use mapreduce.job.user.name....2012-03-21 22:32:58,123  WARN Configuration:661 - fs.default.name is deprecated. Instead, use fs.defaultFS...2012-03-21 22:32:58,130  WARN Configuration:661 - mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address2012-03-21 22:32:58,351  WARN Configuration:345 - fs.default.name is deprecated. Instead, use fs.defaultFS...2012-03-21 22:32:58,843  WARN Configuration:661 - user.name is deprecated. Instead, use mapreduce.job.user.name2012-03-21 22:32:58,844  WARN Configuration:661 - mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address2012-03-21 22:32:58,844  WARN Configuration:661 - fs.default.name is deprecated. Instead, use fs.defaultFS{code}","wordsAmount":49,"comments":[{"name":"verified visually that log WARNs are written once","sentiment":-0.1027},{"name":"+1, patch looks good.\r\n\r\nOne optional nitpick though: Would {{warnOnceIfDeprecated}} be a better method name?","sentiment":0.7003},{"name":"same patch renaming method per Harsh's suggestion","sentiment":0},{"name":"Committed to trunk and branch-0.23","sentiment":0.2732},{"name":"Pulled this into branch-0.23","sentiment":0}],"sentiment":0.7557,"terms":[]}],["avoid linker's stripping of dead code from interfering with configure's library name resolution",{"content":"The configure script generated by hadoop-common/hadoop-common-project/hadoop-common/src/main/native/configure.ac uses the AC_COMPUTE_NEEDED_DSO m4 macro to generate a small probe program in C which it then compiles and links, and then scans the resultant binary to find the names of certain libraries: currently this is used for Zlib and Snappy.I was unable to compile with -Pnative on my Ubuntu Linux install because configure could not find libz and libsnappy. This turned out to be because the linker is removing the dependencies on libz and libsnappy at link-time because the libraries in question are not used in the simple probe code generated by the AC_COMPUTE_NEEDED_DSO m4 macro.So my fix is modify the AC_COMPUTE_NEEDED_DSO to take another parameter that copies the given argument's text into the C program. Then, in the call to AC_COMPUTE_NEEDED_DSO, we can use this additional parameter, to include, in the generated C code, an actual library function call for each library. This prevents the linker from removing the linkage to the desired libraries.My gcc and ldd version information are given below:{code}eugene@latitude:~/hadoop-common$ gcc --versiongcc (Ubuntu/Linaro 4.6.3-1ubuntu3) 4.6.3Copyright (C) 2011 Free Software Foundation, Inc.This is free software; see the source for copying conditions.  There is NOwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.eugene@latitude:~/hadoop-common$ ld --versionGNU ld (GNU Binutils for Ubuntu) 2.22Copyright 2011 Free Software Foundation, Inc.This program is free software; you may redistribute it under the terms ofthe GNU General Public License version 3 or (at your option) a later version.This program has absolutely no warranty.{code}","wordsAmount":143,"comments":[{"name":"Can you check whether it is a dupe of HADOOP-7868?","sentiment":-0.3612},{"name":"Thanks Kihwal, you are right. Also, the solution is better at HADOOP-7868. Marking this as dup.","sentiment":0.7964}],"sentiment":0.952,"terms":["configure script"]}],["Backport FileContext to branch-1",{"content":"It's hard to get users to migrate to FileContext because they (rightly!) want their software to work against Hadoop 1.x which only has FileSystem. Backporting FileContext to branch-1 would allow people to migrate to FileContext and still work against both Hadoop 1.x and 2.x (or whatever we call it). It probably isn't that much work since FileContext is mostly net new code with lots of tests. It's a pain to support the same code in two places, but we already have to do that with FileSystem, and that's the cost of introducing a new API instead of improving FileSystem in place.","wordsAmount":43,"comments":[{"name":"Does this mean the sym link support will be backported as well?","sentiment":0.5859},{"name":"This would get the client side but not the server side, which would need to be backported as well. I'm not planning to do this jira at the moment btw which is why I left it unassigned.","sentiment":0.3919},{"name":"Moved to 1.2.0 upon release of 1.1.0.","sentiment":0},{"name":"Changed Target Version to 1.3.0 upon release of 1.2.0. Please change to 1.2.1 if you intend to submit a fix for branch-1.2.","sentiment":0.3182}],"sentiment":0.6239,"terms":[]}],["viewfs: quota command does not report remaining quotas",{"content":"The space and namesapce quotas and remaining are not reported.","wordsAmount":15,"comments":[{"name":"Uploading a patch. I will file a separate JIRA for tests since the tests are part of hdfs.","sentiment":0},{"name":"Hey John, could you please set affects/target versions for this? Thanks a lot.","sentiment":0.6883},{"name":"Uploading a new patch with a small new test and a fix.","sentiment":0},{"name":"Going to dupe this to HADOOP-8014. Will post an updated patch there.","sentiment":-0.3612},{"name":"Duping this to HADOOP-8014.","sentiment":0}],"sentiment":0,"terms":[]}],["LdapGroupsMapping shouldn't throw away IOException",{"content":"When extracting a password from a file during the config setup, the LdapGroupsMapping throws a RuntimeException, but doesn't bubble up the IOException that caused it","wordsAmount":18,"comments":[{"name":"Patch looks good to me. One tiny nit:\r\n\r\n# Please rename the exception variable to \"ioe\", as is standard in the Hadoop code.\r\n\r\n+1 once this is addressed and Jenkins comes back clean.","sentiment":0.7845},{"name":"Attaching patch that renames exception variable to ioe","sentiment":0},{"name":"I've just committed this to trunk and branch-0.23. Thanks a lot, Natty.","sentiment":0.6124}],"sentiment":0,"terms":[]}],["Fix the build process to do with jsvc, with IBM's JDK as the underlying jdk",{"content":"When IBM JDK is used as the underlying JDK for the build process, the build of jsvc fails. I just needed to add an extra \"os arch\" expression in the condition that sets os-arch.","wordsAmount":15,"comments":[{"name":"Does this change not also need to be made on trunk?","sentiment":0},{"name":"@Aaron, I don't think so. The trunk version deals with jsvc quite differently (HDFS-2303).  ","sentiment":0},{"name":"+1 patch looks good","sentiment":0.4404},{"name":"Committed the patch.","sentiment":0.2732},{"name":"Closed upon release of Hadoop-1.0.3.","sentiment":0}],"sentiment":-0.4215,"terms":["build"]}],["Stop using \"mapred.used.genericoptionsparser\" to avoid unnecessary warnings",{"content":"Its about time we stopped the following from appearing in 0.23/trunk:{code}12/03/19 20:53:51 WARN conf.Configuration: mapred.used.genericoptionsparser is deprecated. Instead, use mapreduce.client.genericoptionsparser.used{code}","wordsAmount":23,"comments":[{"name":"+1","sentiment":0},{"name":"Thanks Todd, committed the trivial change to branch-0.23 and trunk.\r\n\r\nOne less warn everytime :-)","sentiment":0.7031},{"name":"I merged this into branch-0.23.","sentiment":0},{"name":"Thanks Jason. Nicer OOB experience :-)","sentiment":0.7964}],"sentiment":-0.4648,"terms":[]}],["hadoop-setup-single-node.sh from DEB fails with chown permission error on Ubuntu 11.10",{"content":"Installed Hadoop 1.0.1 from amd64 DEB package.Executed: sudo /usr/sbin/hadoop-setup-single-node.shIt invokes /usr/sbin/hadoop-daemon.sh where in line 98chown $HADOOP_IDENT_STRING $HADOOP_LOG_DIRattempts to execute 'chown root /var/log/hadoop/root' as user hduser, resulting in  * Starting Apache Hadoop Name Node server hadoop-namenode                                                                                                     chown: changing ownership of `/var/log/hadoop/root': Operation not permittedand * Starting Apache Hadoop Data Node server hadoop-datanode                                                                                                     chown: changing ownership of `/var/log/hadoop/root': Operation not permittedAs a result, setup fails.","wordsAmount":66,"comments":[{"name":"It seems that for temporary fix you may do:\r\nsudo chmod g+w /var/log/hadoop/root/\r\nsudo chmod g+w /var/log/hadoop/root/SecurityAuth.audit ","sentiment":0}],"sentiment":-0.4215,"terms":["fails"]}],["Review trunk and v0.23 build specs for HADOOP-8037 and HADOOP-8090",{"content":"In branch-1, found and fixed following issues:* binary tarball target was incorrectly persisted from the rpm-generating dir structure, but should have been regular tarball dir structure minus docs and sources* rpm-generating dir structure may be persisted as bin-tarball with platform-specific tag (like rpms)* native libs built for 64-bit platform should go in \"lib64\" rather than \"lib\" directory* platform-specific tags for rpms and debs (and bin-tarballs as above) should be \"i86_64\", not \"amd64\"* consider whether \"i86_64\" should be used in certain directory paths, too.Need to check trunk / v0.23 for related issues, although they'll be different due to different build env.","wordsAmount":60,"comments":[{"name":"Also check for jsvc platform-specific naming issue, HADOOP-8132.","sentiment":0}],"sentiment":0.3919,"terms":["build"]}],["Remove hsqldb since its not needed from pom.xml",{"content":"Related to MAPREDUCE-3621","wordsAmount":8,"comments":[{"name":"Removing unused hsqldb dependencies from pom","sentiment":0},{"name":"The patch applies fine in my dev area. Started jenkins validation again.","sentiment":0.2023},{"name":"+1 for the patch.","sentiment":0},{"name":"Jenkins is failing due to the top level hadoop-project/pom.xml change.  Patch applies fine.  I'll commit this.\r\n\r\nThanks Ravi and thanks for reviewing Suresh.","sentiment":0.7766},{"name":"committed to trunk, branch-2, and branch-0.23","sentiment":0.2732},{"name":"Sorry, this commit broke the build, it needs the corresponding MAPREDUCE-3621, I'm committing that shortly.","sentiment":-0.1531}],"sentiment":0,"terms":[]}],["Disambiguate the destination of FsShell copies",{"content":"The copy commands currently do not provide a way to disambiguate the expected destination of a copy.  Ex.{{fs -put myfile path/mydir}}If \"mydir\" is an existing directory, then the copy produces {{path/mydir/file}}.  If \"mydir\" does not exist at all, the copy produces {{path/mydir}}.  The file has unexpectedly been renamed to what was expected to be a directory!  It's standard unix shell behavior, but it lacks a special trait.Unix allows a user to disambiguate their intent by allowing {{path/mydir/}} or {{path/mydir/.}} to mean \"mydir\" is *always* be treated as a directory.  If the copy succeeds, it will always be called {{path/mydir/myfile}}.","wordsAmount":49,"comments":[{"name":"Thanks Daryn.  I looked over the patch and it looks good +1.  I just put this into trunk, branch-0.23 and branch-0.23.2","sentiment":0.7003}],"sentiment":0.8459,"terms":[]}],["Add mkdir -p flag",{"content":"Users require the ability to create a directory iff it doesn't exist.  Currently, running FsShell's mkdir twice will result in an already exists error.  The unix mkdir -p flag won't fail if the directory already exists.  Per parent jira, pig project would also like this feature.(Eventually mkdir should probably also stop creating all intermediate dirs unless -p is given, but that's an incompatible change)","wordsAmount":36,"comments":[{"name":"I like the patch.  The tests are part of HDFS-3104, I looked at them too and I think they look good.  +1.  But I am not an HDFS committer, just MapReduce and common, so it might be best to get an HDFS committer to put in both.","sentiment":0.8591},{"name":"Oh 1 other thing which is very minor,  You have a TODO in your code.  Please be sure to file a follow up JIRA for it, or remove the TODO.","sentiment":0.5574},{"name":"> ... But I am not an HDFS committer, ...\r\n\r\nHey Robert, this indeed belongs to COMMON since FileSystem and FsShell are in COMMON.  Please feel free to commit it next time.\r\n\r\n+1 patch looks good.","sentiment":0.9331},{"name":"Oops, we forgot to update the usage and description of mkdir.","sentiment":0},{"name":"Update usage & description.","sentiment":0},{"name":"Update usage test.  Didn't realize they weren't in hdfs anymore, kudos to whoever moved them!","sentiment":0.5562},{"name":"+1 The latest patch look good.\r\n\r\nI have committed this.  Thanks, Daryn!","sentiment":0.8256}],"sentiment":0.4633,"terms":[]}],["javadoc generation fails with java.lang.OutOfMemoryError: Java heap space",{"content":"building the docs (mvn package -Pdocs -Dtar -DskipTests) on branch-0.23 results in a javadoc java.lang.OutOfMemoryError: Java heap space. Note this seems to only happen when building with 32 bit java, 64 bit works fine.","wordsAmount":21,"comments":[{"name":"set maxmemory to 512m","sentiment":0},{"name":"patch it to the hadoop-project-dist directory so jenkins doens't apply it correctly.","sentiment":0},{"name":"Thanks Tom.  +1 the patch is very small and I verified that the build still works.  I checked this into trunk and branch-0.23.","sentiment":0.4404}],"sentiment":0.2023,"terms":["fails"]}],["empty-string owners or groups causes {{MissingFormatWidthException}} in o.a.h.fs.shell.Ls.ProcessPath()",{"content":"In {{adjustColumnWidths()}}, we set the member variable {{lineFormat}}, which is used by {{ProcessPath()}} to print directory entries. Owners and groups are formatted using the formatting conversion {{%-Xs}}, where X is the max length of the owner or group. However, when trying this with an S3 URL, I found that the owner and group were empty (\"\"). This caused X to be 0, which means that the formatting conversion is set to {{%-0s}}. This caused a {{MissingFormatWidthException}} to be thrown when the formatting string was used in {{ProcessPath()}}. Formatting conversions are described here: http://docs.oracle.com/javase/1.6.0/docs/api/java/util/Formatter.html#intFlagsThe specific exception thrown (a subtype of {{IllegalFormatException}}) is described here:http://docs.oracle.com/javase/1.6.0/docs/api/java/util/MissingFormatWidthException.html","wordsAmount":66,"comments":[{"name":"Make sure that for {{%-Xs}} formatting conversions, {{X}} is greater than 0.","sentiment":0.5859},{"name":"Note that a similar 0-length check for groups and owners is present in Hadoop 1.0:\r\n\r\nhttps://github.com/apache/hadoop-common/blob/branch-1.0/src/core/org/apache/hadoop/fs/FsShell.java#L645","sentiment":0},{"name":"Sorry, correct line number is :\r\n \r\nhttps://github.com/apache/hadoop-common/blob/branch-1.0/src/core/org/apache/hadoop/fs/FsShell.java#L628","sentiment":0},{"name":"HADOOP-4335 seems to be the same bug.","sentiment":0},{"name":"I think you meant {{Math.min}}.  Although I'd suggest maybe something like this to avoid spurious whitespace:\r\n{code}\r\nfmt.append((maxOwner > 0) ? \"%-\" + maxOwner + \"s \" : \"%s\");\r\n{code}\r\n","sentiment":0.0772},{"name":"Ignore the -Math.min-. I'm tired.","sentiment":-0.6597},{"name":"Hi Daryn, Your fix looks a bit cleaner than mine, thanks!\r\n-Eugene","sentiment":0.5983},{"name":"+1, built & tested with S3","sentiment":0},{"name":"Eugene, just noticed you did not update the patch to Daryn's suggestion. Would you mind uploading a new patch?","sentiment":0},{"name":"Eugene, I'm uploading a patch tweaking your patch to Daryn's comments. Hope this is OK with you.\r\n\r\nI've tested with local/hdfs/s3 filesystem.","sentiment":0.7034},{"name":"+1 looks good to me","sentiment":0.4404},{"name":"Thanks Eugene. Committed to trunk and branch-2.","sentiment":0.6124}],"sentiment":-0.2023,"terms":["string"]}]],"terms":[["given string",{"pos":null,"docs":["NetUtils#getHostNameOfIP blows up if given ip:port string w/o port"],"topic":0,"topicRelation":0.19265985124851537,"ner":null}],["blow given",{"pos":null,"docs":["NetUtils#getHostNameOfIP blows up if given ip:port string w/o port"],"topic":0,"topicRelation":0.12843990083234366,"ner":null}],["gethostnameofip blow",{"pos":null,"docs":["NetUtils#getHostNameOfIP blows up if given ip:port string w/o port"],"topic":0,"topicRelation":0.12843990083234366,"ner":null}],["netutils gethostnameofip",{"pos":null,"docs":["NetUtils#getHostNameOfIP blows up if given ip:port string w/o port"],"topic":0,"topicRelation":0.12843990083234366,"ner":null}],["because regex",{"pos":null,"docs":["NetUtils#getHostNameOfIP blows up if given ip:port string w/o port"],"topic":0,"topicRelation":0.06421995041617183,"ner":null}],["blow",{"pos":null,"docs":["NetUtils#getHostNameOfIP blows up if given ip:port string w/o port"],"topic":0,"topicRelation":0.06421995041617183,"ner":null}],["check given",{"pos":null,"docs":["NetUtils#getHostNameOfIP blows up if given ip:port string w/o port"],"topic":0,"topicRelation":0.06421995041617183,"ner":null}],["doesn't check",{"pos":null,"docs":["NetUtils#getHostNameOfIP blows up if given ip:port string w/o port"],"topic":0,"topicRelation":0.06421995041617183,"ner":null}],["gethostnameofip",{"pos":null,"docs":["NetUtils#getHostNameOfIP blows up if given ip:port string w/o port"],"topic":0,"topicRelation":0.06421995041617183,"ner":null}],["given",{"pos":null,"docs":["NetUtils#getHostNameOfIP blows up if given ip:port string w/o port"],"topic":0,"topicRelation":0.06421995041617183,"ner":null}],["match optionally",{"pos":null,"docs":["NetUtils#getHostNameOfIP blows up if given ip:port string w/o port"],"topic":0,"topicRelation":0.06421995041617183,"ner":null}],["netutils",{"pos":null,"docs":["NetUtils#getHostNameOfIP blows up if given ip:port string w/o port"],"topic":0,"topicRelation":0.06421995041617183,"ner":null}],["optionally requires",{"pos":null,"docs":["NetUtils#getHostNameOfIP blows up if given ip:port string w/o port"],"topic":0,"topicRelation":0.06421995041617183,"ner":null}],["regex match",{"pos":null,"docs":["NetUtils#getHostNameOfIP blows up if given ip:port string w/o port"],"topic":0,"topicRelation":0.06421995041617183,"ner":null}],["requires semicolon",{"pos":null,"docs":["NetUtils#getHostNameOfIP blows up if given ip:port string w/o port"],"topic":0,"topicRelation":0.06421995041617183,"ner":null}],["semicolon doesn't",{"pos":null,"docs":["NetUtils#getHostNameOfIP blows up if given ip:port string w/o port"],"topic":0,"topicRelation":0.06421995041617183,"ner":null}],["string because",{"pos":null,"docs":["NetUtils#getHostNameOfIP blows up if given ip:port string w/o port"],"topic":0,"topicRelation":0.06421995041617183,"ner":null}],["fails",{"pos":null,"docs":["TestZKFailoverController fails","hadoop-setup-single-node.sh from DEB fails with chown permission error on Ubuntu 11.10","javadoc generation fails with java.lang.OutOfMemoryError: Java heap space"],"topic":1,"topicRelation":0.3334380743871628,"ner":null}],["testzkfailovercontroller fails",{"pos":null,"docs":["TestZKFailoverController fails"],"topic":1,"topicRelation":0.4923347634759729,"ner":null}],["testzkfailovercontroller",{"pos":null,"docs":["TestZKFailoverController fails"],"topic":1,"topicRelation":0.4923347634759729,"ner":null}],["string",{"pos":null,"docs":["NetUtils#getHostNameOfIP blows up if given ip:port string w/o port","TestGetBlocks is asserting on a wrong string","empty-string owners or groups causes {{MissingFormatWidthException}} in o.a.h.fs.shell.Ls.ProcessPath()"],"topic":2,"topicRelation":0.07700784272482647,"ner":null}],["asserting wrong",{"pos":null,"docs":["TestGetBlocks is asserting on a wrong string"],"topic":2,"topicRelation":0.22724156689642855,"ner":null}],["testgetblocks asserting",{"pos":null,"docs":["TestGetBlocks is asserting on a wrong string"],"topic":2,"topicRelation":0.22724156689642855,"ner":null}],["wrong string",{"pos":null,"docs":["TestGetBlocks is asserting on a wrong string"],"topic":2,"topicRelation":0.22724156689642855,"ner":null}],["asserting",{"pos":null,"docs":["TestGetBlocks is asserting on a wrong string"],"topic":2,"topicRelation":0.11362078344821427,"ner":null}],["getblockswithexception resulting",{"pos":null,"docs":["TestGetBlocks is asserting on a wrong string"],"topic":2,"topicRelation":0.11362078344821427,"ner":null}],["resulting failure",{"pos":null,"docs":["TestGetBlocks is asserting on a wrong string"],"topic":2,"topicRelation":0.11362078344821427,"ner":null}],["string getblockswithexception",{"pos":null,"docs":["TestGetBlocks is asserting on a wrong string"],"topic":2,"topicRelation":0.11362078344821427,"ner":null}],["testgetblocks",{"pos":null,"docs":["TestGetBlocks is asserting on a wrong string"],"topic":2,"topicRelation":0.11362078344821427,"ner":null}],["wrong",{"pos":null,"docs":["TestGetBlocks is asserting on a wrong string"],"topic":2,"topicRelation":0.11362078344821427,"ner":null}],["common version",{"pos":null,"docs":["Update commons-net version to 3.1"],"topic":3,"topicRelation":0.1969495006292882,"ner":null}],["common",{"pos":null,"docs":["Update commons-net version to 3.1"],"topic":3,"topicRelation":0.0984747503146441,"ner":null}],["hadoop requires",{"pos":null,"docs":["Update commons-net version to 3.1"],"topic":3,"topicRelation":0.0984747503146441,"ner":null}],["latest stable",{"pos":null,"docs":["Update commons-net version to 3.1"],"topic":3,"topicRelation":0.0984747503146441,"ner":null}],["let's latest",{"pos":null,"docs":["Update commons-net version to 3.1"],"topic":3,"topicRelation":0.0984747503146441,"ner":null}],["other ftpfs",{"pos":null,"docs":["Update commons-net version to 3.1"],"topic":3,"topicRelation":0.0984747503146441,"ner":null}],["requires common",{"pos":null,"docs":["Update commons-net version to 3.1"],"topic":3,"topicRelation":0.0984747503146441,"ner":null}],["stable version",{"pos":null,"docs":["Update commons-net version to 3.1"],"topic":3,"topicRelation":0.0984747503146441,"ner":null}],["update common",{"pos":null,"docs":["Update commons-net version to 3.1"],"topic":3,"topicRelation":0.0984747503146441,"ner":null}],["update",{"pos":null,"docs":["Update commons-net version to 3.1"],"topic":3,"topicRelation":0.0984747503146441,"ner":null}],["upgraded let's",{"pos":null,"docs":["Update commons-net version to 3.1"],"topic":3,"topicRelation":0.0984747503146441,"ner":null}],["version other",{"pos":null,"docs":["Update commons-net version to 3.1"],"topic":3,"topicRelation":0.0984747503146441,"ner":null}],["version upgraded",{"pos":null,"docs":["Update commons-net version to 3.1"],"topic":3,"topicRelation":0.0984747503146441,"ner":null}],["version",{"pos":null,"docs":["Update commons-net version to 3.1"],"topic":3,"topicRelation":0.0984747503146441,"ner":null}],["configure script",{"pos":null,"docs":["create the configure script for native compilation as part of the build","avoid linker's stripping of dead code from interfering with configure's library name resolution"],"topic":4,"topicRelation":0.08405373016498333,"ner":null}],["build",{"pos":null,"docs":["create the configure script for native compilation as part of the build","Fix the build process to do with jsvc, with IBM's JDK as the underlying jdk","Review trunk and v0.23 build specs for HADOOP-8037 and HADOOP-8090"],"topic":4,"topicRelation":0.0359025380852429,"ner":null}],["during build",{"pos":null,"docs":["create the configure script for native compilation as part of the build"],"topic":4,"topicRelation":0.10545394198135272,"ner":null}],["script",{"pos":null,"docs":["make hadoop script recognize a full set of deprecated commands","create the configure script for native compilation as part of the build"],"topic":4,"topicRelation":0.04218565633520069,"ner":null}]],"topicsLimit":15,"termsMaxAmount":50}